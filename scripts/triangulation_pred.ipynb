{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f4decc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "from shapely.geometry import Point, Polygon\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import DBSCAN\n",
    "from collections import deque\n",
    "from utils import *\n",
    "\n",
    "from sklearn.neighbors import KDTree\n",
    "from skimage.measure import EllipseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38a9de63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder: Already implemented externally\n",
    "def load_coco_inferences(coco_json_path: str) -> Dict[int, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Load inferenced COCO file and return a mapping from image_id to list of detections.\n",
    "\n",
    "    Args:\n",
    "        coco_json_path (str): Path to the COCO-format JSON file.\n",
    "\n",
    "    Returns:\n",
    "        Dict[int, List[Dict]]: Mapping from image_id to list of detection dicts.\n",
    "    \"\"\"\n",
    "    import json\n",
    "\n",
    "    with open(coco_json_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    # Build mapping from image_id to image file name (if needed)\n",
    "    image_id_to_filename = {img['id']: img['file_name'] for img in coco_data.get('images', [])}\n",
    "\n",
    "    # Group annotations by image_id\n",
    "    image_id_to_anns = defaultdict(list)\n",
    "    for ann in coco_data.get('annotations', []):\n",
    "        image_id_to_anns[ann['image_id']].append(ann)\n",
    "\n",
    "    return dict(image_id_to_anns)\n",
    "\n",
    "\n",
    "def project_ellipse_center_to_world(camera_meta: np.ndarray, ellipse_center_px: np.ndarray) -> Dict:\n",
    "    \"\"\"\n",
    "    Project ellipse center in image to a ray in world coordinates.\n",
    "    Returns {'origin': np.ndarray, 'direction': np.ndarray}\n",
    "    \"\"\"\n",
    "    \n",
    "    film_coords = spherical_unprojection(ellipse_center_px[0], ellipse_center_px[1], 1, camera_meta['width'], camera_meta['height'])\n",
    "    x_local, y_local, z_local = transform_to_local_crs(film_coords[0], film_coords[1], film_coords[2], camera_meta)\n",
    "    ray_vertex = np.array([x_local, y_local, z_local])\n",
    "\n",
    "    cam_center = np.array([camera_meta['x'], camera_meta['y'], camera_meta['z']])\n",
    "    ray_ori = ray_vertex - cam_center\n",
    "    ray_data = {\n",
    "        'origin': cam_center,\n",
    "        'direction': ray_ori\n",
    "    }\n",
    "    return ray_data\n",
    "\n",
    "def spatial_temporal_group_sort(\n",
    "    gdf_grouped,\n",
    "    time_column=\"gps_sec_s_\",\n",
    "    x_col=\"x_m_\",\n",
    "    y_col=\"y_m_\",\n",
    "    z_col=\"z_m_\",\n",
    "    radius=10.0,\n",
    "    output_column=\"sort_index\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Two-stage spatial-temporal sort on groupby(image_id) object.\n",
    "    \n",
    "    Parameters:\n",
    "        gdf_grouped: GroupBy object, grouped by 'image_id'.\n",
    "        time_column (str): Column for GPS time (shared within each group).\n",
    "        x_col, y_col, z_col (str): Position columns (shared within each group).\n",
    "        radius (float): Search radius in meters.\n",
    "        output_column (str): Name of output sort index column.\n",
    "    \n",
    "    Returns:\n",
    "        GeoDataFrame: With new sort_index column.\n",
    "        GroupBy: Sorted groupby object (ordered by sort_index).\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Extract one row per group to represent its position and time\n",
    "    group_meta = (\n",
    "        gdf_grouped[[time_column, x_col, y_col, z_col]]\n",
    "        .first()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Sort groups by time (Stage 1)\n",
    "    group_meta = group_meta.sort_values(by=time_column).reset_index(drop=True)\n",
    "    positions = group_meta[[x_col, y_col, z_col]].values\n",
    "    image_ids = group_meta[\"image_id\"].values\n",
    "\n",
    "    tree = KDTree(positions)\n",
    "    used = np.zeros(len(group_meta), dtype=bool)\n",
    "    final_order_indices = []\n",
    "\n",
    "    prev_anchor_idx = None\n",
    "\n",
    "    for current_anchor_idx in range(len(group_meta)):\n",
    "        if used[current_anchor_idx]:\n",
    "            continue\n",
    "\n",
    "        current_pos = positions[current_anchor_idx]\n",
    "\n",
    "        if prev_anchor_idx is None:\n",
    "            group_indices = [current_anchor_idx]\n",
    "        else:\n",
    "            prev_pos = positions[prev_anchor_idx]\n",
    "            direction = current_pos - prev_pos\n",
    "            direction_norm = np.linalg.norm(direction)\n",
    "            if direction_norm < 1e-6:\n",
    "                direction = np.array([1, 0, 0])\n",
    "            else:\n",
    "                direction = direction / direction_norm\n",
    "\n",
    "            neighbor_indices = tree.query_radius([current_pos], r=radius)[0]\n",
    "            neighbor_indices = [i for i in neighbor_indices if not used[i]]\n",
    "\n",
    "            if not neighbor_indices:\n",
    "                continue\n",
    "\n",
    "            neighbor_positions = positions[neighbor_indices]\n",
    "            projections = neighbor_positions @ direction\n",
    "            sorted_local = np.array(neighbor_indices)[np.argsort(projections)]\n",
    "            group_indices = sorted_local.tolist()\n",
    "\n",
    "        used[group_indices] = True\n",
    "        final_order_indices.extend(group_indices)\n",
    "        prev_anchor_idx = current_anchor_idx\n",
    "\n",
    "    # Create mapping from image_id to sort index\n",
    "    sort_index_map = {image_ids[idx]: i for i, idx in enumerate(final_order_indices)}\n",
    "\n",
    "    # Apply mapping to original GeoDataFrame\n",
    "    gdf_with_sort = gdf_grouped.obj.copy()\n",
    "    gdf_with_sort[output_column] = gdf_with_sort[\"image_id\"].map(sort_index_map)\n",
    "\n",
    "    # Return new dataframe and sorted groupby object\n",
    "    gdf_sorted = gdf_with_sort.sort_values(by=output_column)\n",
    "    return gdf_sorted.groupby(\"image_id\", sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a4064f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ellipse_from_mask(mask: np.ndarray):\n",
    "    \"\"\"Fit an ellipse to a binary mask.\"\"\"\n",
    "    contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return None\n",
    "    cnt = max(contours, key=cv2.contourArea)\n",
    "    if len(cnt) < 5:\n",
    "        return None\n",
    "    ellipse = cv2.fitEllipse(cnt)\n",
    "    return ellipse  # (center(x, y), (major, minor), angle)\n",
    "\n",
    "class Ray():\n",
    "    def __init__(self, origin: np.ndarray, direction: np.ndarray, frame_id: int, candidate_id=None):\n",
    "        self.origin = origin\n",
    "        self.direction = direction / np.linalg.norm(direction)\n",
    "        self.frame_id = frame_id\n",
    "        self.candidate_id = candidate_id  # which candidate this ray is currently associated with\n",
    "\n",
    "# Intersection class definition\n",
    "class Intersection:\n",
    "    def __init__(self, point, ray_pair, dist, length):\n",
    "        self.point = point          # np.array([x, y, z])\n",
    "        self.ray_pair = ray_pair    # tuple of (prev_idx, new_idx)\n",
    "        self.dist = dist            # float, distance between rays at intersection\n",
    "        self.length = length        # float, sum of distances from ray origins to intersection\n",
    "\n",
    "def compute_ray_intersection(ray1: Ray, ray2: Ray, radius: float = 10.0, distance_threshold: float = 0.5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return intersection point if rays intersect in their forward direction and are close enough.\n",
    "    Otherwise, return None.\n",
    "    \"\"\"\n",
    "    p1, d1 = ray1.origin, ray1.direction\n",
    "    p2, d2 = ray2.origin, ray2.direction\n",
    "\n",
    "    v12 = p2 - p1\n",
    "    d1_dot_d2 = np.dot(d1, d2)\n",
    "    denom = 1 - d1_dot_d2 ** 2\n",
    "    if abs(denom) < 1e-6:\n",
    "        return None, None, None  # Nearly parallel, no intersection in forward direction\n",
    "    t1 = (np.dot(v12, d1) - np.dot(v12, d2) * d1_dot_d2) / denom\n",
    "    t2 = - (np.dot(v12, d2) - np.dot(v12, d1) * d1_dot_d2) / denom\n",
    "    if t1 < 0 or t2 < 0 or t1 > radius or t2 > radius:\n",
    "        return None, None, None # Closest approach is behind at least one ray's origin\n",
    "    point1 = p1 + t1 * d1\n",
    "    point2 = p2 + t2 * d2\n",
    "    dist = np.linalg.norm(point1 - point2)\n",
    "\n",
    "    if dist > distance_threshold:\n",
    "        return None, None, None  # Closest points are too far apart to be considered an intersection\n",
    "    \n",
    "    intersect = (point1 + point2) / 2\n",
    "    if intersect[2] >= min(p1[2], p2[2]) - 2.0 or intersect[2] <= min(p1[2], p2[2]) - 3.8:\n",
    "        return None, None, None\n",
    "    return intersect, dist, max(t1, t2)\n",
    "\n",
    "def cluster_intersections(intersections: List[np.ndarray], distance_threshold: float = 0.5) -> List[List[int]]:\n",
    "    \"\"\"Cluster intersection points by spatial proximity using DBSCAN.\"\"\"\n",
    "    if len(intersections) == 0:\n",
    "        return []\n",
    "    X = np.stack(intersections)\n",
    "    db = DBSCAN(eps=distance_threshold, min_samples=1).fit(X)\n",
    "    labels = db.labels_\n",
    "    clusters = []\n",
    "    for label in set(labels):\n",
    "        cluster = np.where(labels == label)[0].tolist()\n",
    "        clusters.append(cluster)\n",
    "    return clusters\n",
    "\n",
    "def estimate_disc_geometry(intersection_points: List[np.ndarray]) -> Dict:\n",
    "    \"\"\"Initialize manhole model: center, upward normal, and radius.\"\"\"\n",
    "    center = np.mean(intersection_points, axis=0)\n",
    "    normal = np.array([0, 0, 1])\n",
    "    radius = 0.3  # fixed 30cm\n",
    "    return {\"center\": center, \"normal\": normal, \"radius\": radius}\n",
    "\n",
    "def detect_and_localize_manhole(gdf: gpd.GeoDataFrame, verbose=True) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Detect and localize manholes from georeferenced mobile mapping imagery.\n",
    "    Uses a list for rays, updates candidates incrementally, and manages candidate/ray lifecycle.\n",
    "    \"\"\"\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fcf497",
   "metadata": {},
   "source": [
    "# Trajectory & COCO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ac6751",
   "metadata": {},
   "source": [
    "## Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f095a0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the three COCO files\n",
    "coco_paths = [\n",
    "    '/mnt/Data/StreetView/data/neuchatel/trn_COCO_panoptic_detections.json',\n",
    "    '/mnt/Data/StreetView/data/neuchatel/val_COCO_panoptic_detections.json',\n",
    "    '/mnt/Data/StreetView/data/neuchatel/tst_COCO_panoptic_detections.json'\n",
    "]\n",
    "\n",
    "merged_file_path = '/mnt/Data/StreetView/data/neuchatel/COCO_panoptic_detections.json'\n",
    "# Load all three COCO files\n",
    "coco_datas = []\n",
    "for path in coco_paths:\n",
    "    with open(path, 'r') as f:\n",
    "        coco_datas.append(json.load(f))\n",
    "\n",
    "# Merge 'images' and remove duplicates by 'file_name'\n",
    "all_images = []\n",
    "seen_file_names = set()\n",
    "for coco in coco_datas:\n",
    "    for img in coco['images']:\n",
    "        fname = img['file_name']\n",
    "        if fname not in seen_file_names:\n",
    "            all_images.append(img)\n",
    "            seen_file_names.add(fname)\n",
    "\n",
    "# Merge 'annotations'\n",
    "all_annotations = []\n",
    "for coco in coco_datas:\n",
    "    all_annotations.extend(coco.get('annotations', []))\n",
    "\n",
    "# Use categories from the first file (assuming all are the same)\n",
    "categories = coco_datas[0].get('categories', [])\n",
    "\n",
    "# Build merged COCO data\n",
    "merged_coco = {\n",
    "    'images': all_images,\n",
    "    'annotations': all_annotations,\n",
    "    'categories': categories\n",
    "}\n",
    "\n",
    "# Remove parent folders from file_name in coco_data['images'] and rename 'image_id' to 'id'\n",
    "for img in merged_coco['images']:\n",
    "    img['file_name'] = img['file_name'].split('/')[-1]\n",
    "    if 'image_id' in img:\n",
    "        img['id'] = img.pop('image_id')\n",
    "\n",
    "merged_coco['categories'] = [{\n",
    "    'id': int(0), \n",
    "    'name': 'manhole', \n",
    "    'supercategory': 'none'}]\n",
    "# Save the modified coco_data back to coco_file_path\n",
    "with open(merged_file_path, 'w') as f:\n",
    "    json.dump(merged_coco, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0250cc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the trajectory CSV as a DataFrame\n",
    "traject = pd.read_csv('/mnt/Data/StreetView/data/neuchatel/ne_traject.csv')\n",
    "coco_file_path = '/mnt/Data/StreetView/data/neuchatel/COCO_panoptic_detections.json'\n",
    "\n",
    "\n",
    "# Construct geometry from x_m_, y_m_, z_m_\n",
    "traject['geometry'] = traject.apply(lambda row: Point(row['x_m_'], row['y_m_'], row['z_m_']), axis=1)\n",
    "gdf = gpd.GeoDataFrame(traject, geometry='geometry', crs=\"epsg:2056\")\n",
    "gdf = gpd.GeoDataFrame(traject)\n",
    "\n",
    "# Load COCO data\n",
    "with open(coco_file_path, 'r') as f:\n",
    "    coco_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67eff013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Match gdf with coco_data by file_name ---\n",
    "# Build a DataFrame from coco_data['images']\n",
    "coco_images_df = pd.DataFrame(coco_data['images'])\n",
    "coco_images_df.id = coco_images_df.id.astype(int)\n",
    "\n",
    "# Remove '.jpg' from coco file_name for matching\n",
    "coco_images_df['file_name_no_ext'] = coco_images_df['file_name'].str.replace('.jpg', '', regex=False)\n",
    "\n",
    "# If gdf has a column with file name (without extension), match on that\n",
    "# Let's assume the column is 'file_name' in gdf (without .jpg)\n",
    "# If not, adjust accordingly\n",
    "if 'file_name' not in gdf.columns:\n",
    "    raise ValueError(\"gdf must have a 'file_name' column to match with COCO images.\")\n",
    "\n",
    "# Merge gdf with coco_images_df to get the COCO image id\n",
    "gdf = gdf.merge(\n",
    "    coco_images_df[['id', 'file_name_no_ext', 'width', 'height']],\n",
    "    left_on='file_name',\n",
    "    right_on='file_name_no_ext',\n",
    "    how='right'\n",
    ")\n",
    "gdf = gdf.rename(columns={'id': 'coco_image_id'})\n",
    "gdf.drop(columns=['file_name_no_ext'], inplace=True)\n",
    "\n",
    "# --- COCO annotation processing as before ---\n",
    "# Prepare a list of annotation records\n",
    "records = []\n",
    "for ann in coco_data['annotations']:\n",
    "    if float(ann['score']) < 0.9 or float(ann['area']) > 10000 or float(ann['area']) < 400:\n",
    "        continue\n",
    "    image_id = ann['image_id']\n",
    "    segmentation = ann['segmentation'][0]\n",
    "\n",
    "    coords = [(segmentation[i], segmentation[i+1]) for i in range(0, len(segmentation), 2)]\n",
    "    geometry = Polygon(coords)\n",
    "    records.append({\n",
    "        'image_id': image_id,\n",
    "        'category_id': ann.get('category_id', None),\n",
    "        'geometry': geometry,\n",
    "        'annotation_id': ann.get('id', None),\n",
    "        'score': ann.get('score', None),\n",
    "        'area': ann.get('area', None)\n",
    "    })\n",
    "\n",
    "# Create a GeoDataFrame\n",
    "ann_gdf = gpd.GeoDataFrame(records, geometry='geometry', crs=\"epsg:2056\")\n",
    "\n",
    "# Merge gdf with ann_gdf to get prediction mask\n",
    "gdf = gdf.merge(\n",
    "    ann_gdf,\n",
    "    left_on='coco_image_id',\n",
    "    right_on='image_id',\n",
    "    how='right'\n",
    ")\n",
    "gdf.drop(columns=['coco_image_id'], inplace=True)\n",
    "\n",
    "grouped = spatial_temporal_group_sort(gdf.groupby('image_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81536a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(8, 4))\n",
    "# ann_gdf['score'].hist(bins=50)\n",
    "# plt.title('Distribution of annotation scores')\n",
    "# plt.xlabel('Score')\n",
    "# plt.ylabel('Count')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe58daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 4))\n",
    "# ann_gdf['area'][ann_gdf['area']<1000].hist(bins=50)\n",
    "# plt.title('Distribution of annotation scores')\n",
    "# plt.xlabel('Score')\n",
    "# plt.ylabel('Count')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12f39c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   0%|          | 0/4459 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|██████████| 4459/4459 [00:49<00:00, 89.21it/s] \n",
      "100%|██████████| 1855/1855 [00:00<00:00, 22845.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "intersection_distance_threshold = 0.5  # meters, for intersection and clustering\n",
    "candidate_update_threshold = 1.0       # meters, for candidate consistency\n",
    "candidate_missing_limit = 5            # number of consecutive frames without new rays before removal\n",
    "radius = 20.0\n",
    "\n",
    "# Candidate class for storing candidate attributes\n",
    "class Candidate:\n",
    "    def __init__(self, center, intersections, mean_intersection_length, mean_intersection_dist, last_seen, missing):\n",
    "        self.center = center\n",
    "        self.intersections = set(intersections)\n",
    "        self.mean_intersection_length = mean_intersection_length\n",
    "        self.mean_intersection_dist = mean_intersection_dist\n",
    "        self.last_seen = last_seen\n",
    "        self.missing = missing\n",
    "\n",
    "    def update(self, center, intersections, mean_intersection_length, mean_intersection_dist, last_seen):\n",
    "        self.center = center\n",
    "        self.intersections = set(intersections)\n",
    "        self.mean_intersection_length = mean_intersection_length\n",
    "        self.mean_intersection_dist = mean_intersection_dist\n",
    "        self.last_seen = last_seen\n",
    "        self.missing = 0\n",
    "\n",
    "    def increment_missing(self):\n",
    "        self.missing += 1\n",
    "\n",
    "    def to_row(self):\n",
    "        center = self.center\n",
    "        return {\n",
    "            'elevation': center[2],\n",
    "            'radius': 0.3,\n",
    "            'normal_x': 0,\n",
    "            'normal_y': 0,\n",
    "            'normal_z': 1,\n",
    "            'intersections': set(self.intersections),\n",
    "            'mean_intersection_length': self.mean_intersection_length,\n",
    "            'mean_intersection_dist': self.mean_intersection_dist,\n",
    "            'geometry': Point(center[0], center[1])\n",
    "        }\n",
    "\n",
    "# State\n",
    "ray_list = []  # List of Ray objects (never removed, indices are constant)\n",
    "candidate_list = []  # List of Candidate objects\n",
    "intersection_objs = []  # List of Intersection objects\n",
    "\n",
    "iterator = tqdm(grouped, total=len(grouped), desc=\"Processing frames\")\n",
    "\n",
    "# Maintain sets of active indices for rays, candidates, and intersections\n",
    "active_ray_indices = set()\n",
    "active_candidate_indices = set()\n",
    "active_intersection_indices = set()\n",
    "\n",
    "for image_id, df_frame in iterator:\n",
    "    frame_id = image_id\n",
    "\n",
    "    camera_meta = {\n",
    "        'x': df_frame.iloc[0]['x_m_'],\n",
    "        'y': df_frame.iloc[0]['y_m_'],\n",
    "        'z': df_frame.iloc[0]['z_m_'],\n",
    "        'yaw': df_frame.iloc[0]['gpsimgdirection'] + 0.5,\n",
    "        'pitch': df_frame.iloc[0]['gpspitch'],\n",
    "        'roll': df_frame.iloc[0]['gpsroll'] - 0.3,\n",
    "        'width': df_frame.iloc[0]['width'],\n",
    "        'height': df_frame.iloc[0]['height']\n",
    "    }\n",
    "\n",
    "    # Get predicted masks for this image \n",
    "    new_rays = []\n",
    "    for _, det in df_frame.iterrows():\n",
    "        # Fit a minimum area ellipse to the contour of det.geometry_y and use its center as center_px\n",
    "        # Get the exterior coordinates of the geometry as (N, 2) array\n",
    "        coords = np.array(det.geometry_y.exterior.coords)\n",
    "        # Fit ellipse to the contour\n",
    "        ellipse = EllipseModel()\n",
    "        success = ellipse.estimate(coords)\n",
    "        if success:\n",
    "            xc, yc, a, b, theta = ellipse.params\n",
    "            center_px = (xc, yc)\n",
    "        else:\n",
    "            # Fallback to centroid if ellipse fit fails\n",
    "            center_px = np.array(det.geometry_y.centroid.coords)[0]\n",
    "            print('ellipse fit fails')\n",
    "        ray_data = project_ellipse_center_to_world(camera_meta, center_px)\n",
    "        ray = Ray(ray_data['origin'], ray_data['direction'], frame_id)\n",
    "        new_rays.append(ray)\n",
    "\n",
    "    # Add new rays to the ray list\n",
    "    ray_start_idx = len(ray_list)\n",
    "    ray_list.extend(new_rays)\n",
    "    new_ray_indices = np.arange(ray_start_idx, len(ray_list))\n",
    "\n",
    "    # When new rays are added, mark their indices as active\n",
    "    active_ray_indices.update(new_ray_indices)\n",
    "\n",
    "    # Compute intersections only between new rays and previous active rays, but store all intersections globally\n",
    "    n_prev_rays = ray_start_idx\n",
    "    n_new_rays = len(new_rays)\n",
    "    new_intersection_indices = []\n",
    "    if n_prev_rays > 0 and n_new_rays > 0:\n",
    "        # For each pair of frame_id, only keep the intersection with the lowest dist\n",
    "        # Use a dict to store: key = (frame_id_prev, frame_id_new), value = (dist, Intersection)\n",
    "        for new_idx, ray_new in zip(new_ray_indices, new_rays):\n",
    "            best_intersections = dict()\n",
    "            for prev_idx in active_ray_indices:\n",
    "                ray_prev = ray_list[prev_idx]\n",
    "                # Only consider pairs from different frames\n",
    "                if ray_prev.frame_id != ray_new.frame_id:\n",
    "                    pt, dist, length = compute_ray_intersection(ray_prev, ray_new, radius=radius, distance_threshold=intersection_distance_threshold)\n",
    "                    if pt is not None:\n",
    "                        # Sort frame_id order to avoid duplicates (frame_id_a, frame_id_b)\n",
    "                        frame_pair = tuple(sorted([ray_prev.frame_id, ray_new.frame_id]))\n",
    "                        if frame_pair not in best_intersections or dist < best_intersections[frame_pair].dist:\n",
    "                            best_intersections[frame_pair] = Intersection(\n",
    "                                point=pt,\n",
    "                                ray_pair=(prev_idx, new_idx),\n",
    "                                dist=dist,\n",
    "                                length=length\n",
    "                            )\n",
    "            # After collecting, add only the best intersections for each frame pair\n",
    "            for intersection in best_intersections.values():\n",
    "                intersection_objs.append(intersection)\n",
    "                new_intersection_indices.append(len(intersection_objs) - 1)\n",
    "    # Add new intersections to active set\n",
    "    active_intersection_indices.update(new_intersection_indices)\n",
    "\n",
    "    # Cluster only active intersections\n",
    "    if len(active_intersection_indices) > 0:\n",
    "        intersection_points = [intersection_objs[i].point for i in active_intersection_indices]\n",
    "        clusters = cluster_intersections(intersection_points, distance_threshold=0.5)\n",
    "        # clusters: list of lists of indices into intersection_points (not global intersection_objs)\n",
    "        # Map cluster indices back to global intersection_objs indices\n",
    "        clusters_global = []\n",
    "        active_intersection_indices_list = list(active_intersection_indices)\n",
    "        for cluster in clusters:\n",
    "            clusters_global.append([active_intersection_indices_list[i] for i in cluster])\n",
    "    else:\n",
    "        clusters_global = []\n",
    "\n",
    "    # For each cluster, use the 3 intersects with least length to calculate candidate\n",
    "    new_candidates = []\n",
    "    if len(clusters_global) > 0:\n",
    "        for cluster in clusters_global:\n",
    "            # cluster is a list of indices into intersection_objs (global indices)\n",
    "            cand_inters = [intersection_objs[i] for i in cluster]\n",
    "            # Sort by length\n",
    "            cand_inters_with_idx = sorted(zip(cluster, cand_inters), key=lambda x: x[1].length)\n",
    "            # Take up to 3 intersections (if less than 3, take all)\n",
    "            top_k = min(3, len(cand_inters_with_idx))\n",
    "            selected_inters = cand_inters_with_idx[:top_k]\n",
    "            selected_points = np.array([inter[1].point for inter in selected_inters])\n",
    "            selected_intersection_indices = set([inter[0] for inter in selected_inters])\n",
    "            selected_lengths = [inter[1].length for inter in selected_inters]\n",
    "            selected_dists = [inter[1].dist for inter in selected_inters]\n",
    "            mean_intersection_length = float(np.mean(selected_lengths)) if selected_lengths else float('nan')\n",
    "            mean_intersection_dist = float(np.mean(selected_dists)) if selected_dists else float('nan')\n",
    "            # Find all ray indices involved in these intersections\n",
    "            cluster_ray_indices = set()\n",
    "            for inter in selected_inters:\n",
    "                cluster_ray_indices.update(inter[1].ray_pair)\n",
    "            # Compute center as mean of the selected points\n",
    "            center = np.mean(selected_points, axis=0)\n",
    "            new_candidates.append(\n",
    "                Candidate(\n",
    "                    center=center,\n",
    "                    intersections=selected_intersection_indices,\n",
    "                    mean_intersection_length=mean_intersection_length,\n",
    "                    mean_intersection_dist=mean_intersection_dist,\n",
    "                    last_seen=frame_id,\n",
    "                    missing=0\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Update existing candidates or add new ones (dynamic pool)\n",
    "    updated_candidate_ids = set()\n",
    "    # Only consider active candidates for matching\n",
    "    active_candidate_indices_list = list(active_candidate_indices)\n",
    "    for nc in new_candidates:\n",
    "        nc_center = nc.center\n",
    "        nc_inters = nc.intersections\n",
    "        nc_mean_length = nc.mean_intersection_length\n",
    "        nc_mean_dist = nc.mean_intersection_dist\n",
    "        matched = False\n",
    "        if len(active_candidate_indices) > 0:\n",
    "            candidate_centers = np.array([candidate_list[cid].center for cid in active_candidate_indices_list])\n",
    "            dists = np.linalg.norm(candidate_centers - nc_center, axis=1)\n",
    "            within_radius_indices = np.where(dists < radius)[0]\n",
    "            candidates_to_compare = [(active_candidate_indices_list[cid], candidate_list[active_candidate_indices_list[cid]]) for cid in within_radius_indices]\n",
    "        else:\n",
    "            candidates_to_compare = []\n",
    "        for cid, cand in candidates_to_compare:\n",
    "            dist = np.linalg.norm(nc_center - cand.center)\n",
    "            if dist != 0 and dist < candidate_update_threshold:\n",
    "                combined_inters = cand.intersections.union(nc_inters)\n",
    "                relevant_inters = [\n",
    "                    (i, intersection_objs[i])\n",
    "                    for i in combined_inters\n",
    "                    if i in active_intersection_indices\n",
    "                ]\n",
    "                if len(relevant_inters) > 0:\n",
    "                    # Sort by length and take up to 3\n",
    "                    relevant_inters_sorted = sorted(relevant_inters, key=lambda x: x[1].length)\n",
    "                    top_k = min(3, len(relevant_inters_sorted))\n",
    "                    selected_points = np.array([relevant_inters_sorted[i][1].point for i in range(top_k)])\n",
    "                    selected_indices = set([relevant_inters_sorted[i][0] for i in range(top_k)])\n",
    "                    selected_lengths = [relevant_inters_sorted[i][1].length for i in range(top_k)]\n",
    "                    selected_dists = [relevant_inters_sorted[i][1].dist for i in range(top_k)]\n",
    "                    new_center = np.mean(selected_points, axis=0)\n",
    "                    mean_intersection_length = float(np.mean(selected_lengths)) if selected_lengths else float('nan')\n",
    "                    mean_intersection_dist = float(np.mean(selected_dists)) if selected_dists else float('nan')\n",
    "                    cand.update(\n",
    "                        center=new_center,\n",
    "                        intersections=selected_indices,\n",
    "                        mean_intersection_length=mean_intersection_length,\n",
    "                        mean_intersection_dist=mean_intersection_dist,\n",
    "                        last_seen=frame_id\n",
    "                    )\n",
    "                else:\n",
    "                    cand.intersections = combined_inters\n",
    "                    lengths = [intersection_objs[i].length for i in combined_inters if i < len(intersection_objs)]\n",
    "                    dists_ = [intersection_objs[i].dist for i in combined_inters if i < len(intersection_objs)]\n",
    "                    cand.mean_intersection_length = float(np.mean(lengths)) if lengths else float('nan')\n",
    "                    cand.mean_intersection_dist = float(np.mean(dists_)) if dists_ else float('nan')\n",
    "                    cand.last_seen = frame_id\n",
    "                    cand.missing = 0\n",
    "                updated_candidate_ids.add(cid)\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            candidate_list.append(\n",
    "                Candidate(\n",
    "                    center=nc_center,\n",
    "                    intersections=set(nc_inters),\n",
    "                    mean_intersection_length=nc_mean_length,\n",
    "                    mean_intersection_dist=nc_mean_dist,\n",
    "                    last_seen=frame_id,\n",
    "                    missing=0\n",
    "                )\n",
    "            )\n",
    "            new_cid = len(candidate_list) - 1\n",
    "            updated_candidate_ids.add(new_cid)\n",
    "            active_candidate_indices.add(new_cid)\n",
    "\n",
    "    # For candidates not updated, increment missing count\n",
    "    for cid in list(active_candidate_indices):\n",
    "        if cid not in updated_candidate_ids:\n",
    "            cand = candidate_list[cid]\n",
    "            cand.increment_missing()\n",
    "            if cand.missing > candidate_missing_limit:\n",
    "                rays_to_remove = set()\n",
    "                for iid in cand.intersections:\n",
    "                    inter = intersection_objs[iid]\n",
    "                    rays_to_remove.update(inter.ray_pair)\n",
    "                active_ray_indices.difference_update(rays_to_remove)\n",
    "                active_candidate_indices.discard(cid)\n",
    "                # Also deactivate intersections that only involve rays from this candidate\n",
    "                intersections_to_remove = set()\n",
    "                for iid in list(active_intersection_indices):\n",
    "                    inter = intersection_objs[iid]\n",
    "                    if inter.ray_pair[0] in rays_to_remove or inter.ray_pair[1] in rays_to_remove:\n",
    "                        intersections_to_remove.add(iid)\n",
    "                active_intersection_indices.difference_update(intersections_to_remove)\n",
    "\n",
    "# Prepare candidate centers for clustering\n",
    "candidate_centers = np.array([cand.center for cand in candidate_list])\n",
    "if len(candidate_centers) == 0:\n",
    "    pass\n",
    "else:\n",
    "    # Cluster with DBSCAN using intersection_distance_threshold as eps\n",
    "    db = DBSCAN(eps=intersection_distance_threshold, min_samples=1)\n",
    "    labels = db.fit_predict(candidate_centers)\n",
    "\n",
    "    # Group candidate indices by cluster label\n",
    "    from collections import defaultdict\n",
    "    clusters = defaultdict(list)\n",
    "    for idx, label in enumerate(labels):\n",
    "        clusters[label].append(idx)\n",
    "\n",
    "    new_candidate_list = []\n",
    "    old_to_new_cid = {}\n",
    "\n",
    "    for label, indices in tqdm(clusters.items()):\n",
    "        if len(indices) == 1:\n",
    "            # Keep singleton candidate as is\n",
    "            orig_cid = indices[0]\n",
    "            new_cid = len(new_candidate_list)\n",
    "            new_candidate_list.append(candidate_list[orig_cid])\n",
    "            old_to_new_cid[orig_cid] = new_cid\n",
    "        else:\n",
    "            # Merge candidates in this cluster\n",
    "            merged_intersections = set()\n",
    "            merged_last_seen = -1\n",
    "            merged_missing = float('inf')\n",
    "            for cid in indices:\n",
    "                cand = candidate_list[cid]\n",
    "                merged_intersections.update(cand.intersections)\n",
    "                merged_last_seen = max(merged_last_seen, cand.last_seen)\n",
    "                merged_missing = min(merged_missing, cand.missing)\n",
    "            intersection_points = []\n",
    "            intersection_lengths = []\n",
    "            intersection_dists = []\n",
    "            for iid in merged_intersections:\n",
    "                intersection_points.append(intersection_objs[iid].point)\n",
    "                intersection_lengths.append(intersection_objs[iid].length)\n",
    "                intersection_dists.append(intersection_objs[iid].dist)\n",
    "            if intersection_points:\n",
    "                # Use up to 3 shortest-length intersections for center and mean\n",
    "                merged_inters_with_length = [(iid, intersection_objs[iid]) for iid in merged_intersections]\n",
    "                merged_inters_with_length_sorted = sorted(merged_inters_with_length, key=lambda x: x[1].length)\n",
    "                top_k = min(3, len(merged_inters_with_length_sorted))\n",
    "                selected_points = np.array([merged_inters_with_length_sorted[i][1].point for i in range(top_k)])\n",
    "                selected_indices = set([merged_inters_with_length_sorted[i][0] for i in range(top_k)])\n",
    "                selected_lengths = [merged_inters_with_length_sorted[i][1].length for i in range(top_k)]\n",
    "                selected_dists = [merged_inters_with_length_sorted[i][1].dist for i in range(top_k)]\n",
    "                new_center = np.mean(selected_points, axis=0)\n",
    "                mean_intersection_length = float(np.mean(selected_lengths)) if selected_lengths else float('nan')\n",
    "                mean_intersection_dist = float(np.mean(selected_dists)) if selected_dists else float('nan')\n",
    "                new_intersections = selected_indices\n",
    "            else:\n",
    "                centers = [candidate_list[k].center for k in indices]\n",
    "                new_center = np.mean(np.array(centers), axis=0)\n",
    "                mean_intersection_length = float('nan')\n",
    "                mean_intersection_dist = float('nan')\n",
    "                new_intersections = set()\n",
    "            new_cand = Candidate(\n",
    "                center=new_center,\n",
    "                intersections=new_intersections if new_intersections else merged_intersections,\n",
    "                mean_intersection_length=mean_intersection_length,\n",
    "                mean_intersection_dist=mean_intersection_dist,\n",
    "                last_seen=merged_last_seen,\n",
    "                missing=merged_missing\n",
    "            )\n",
    "            new_cid = len(new_candidate_list)\n",
    "            for k in indices:\n",
    "                old_to_new_cid[k] = new_cid\n",
    "            new_candidate_list.append(new_cand)\n",
    "\n",
    "    # Replace candidate_list with merged version\n",
    "    candidate_list = new_candidate_list\n",
    "\n",
    "    # Update active_candidate_indices to new indices\n",
    "    active_candidate_indices = set(old_to_new_cid[cid] for cid in active_candidate_indices if cid in old_to_new_cid)\n",
    "\n",
    "# Output remaining active candidates as GeoDataFrame\n",
    "rows = []\n",
    "for cid, cand in enumerate(candidate_list):\n",
    "    row = cand.to_row()\n",
    "    rows.append(row)\n",
    "\n",
    "out_gdf = gpd.GeoDataFrame(rows, geometry='geometry', crs=\"epsg:2056\")\n",
    "out_gdf.to_file(f\"triangulation_points_pred_{str(int(radius))+'m'}.gpkg\", driver=\"GPKG\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16421a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization function for candidates, rays, and intersections using Open3D ---\n",
    "\n",
    "def visualize_candidates_open3d(candidate_indices, ray_list=ray_list, candidate_list=candidate_list, intersection_objs=intersection_objs):\n",
    "    \"\"\"\n",
    "    Visualize rays and intersections that construct the given candidates using Open3D.\n",
    "    The visualization origin is shifted to the first ray origin used by the first candidate.\n",
    "    Args:\n",
    "        candidate_indices: list of candidate indices to visualize\n",
    "        ray_list: list of Ray objects\n",
    "        candidate_list: list of Candidate objects\n",
    "        intersection_objs: list of Intersection objects\n",
    "    \"\"\"\n",
    "    import open3d as o3d\n",
    "    import numpy as np\n",
    "\n",
    "    # Collect all rays and intersection points used by the selected candidates\n",
    "    all_ray_indices = set()\n",
    "    all_intersection_points = []\n",
    "    candidate_centers = []\n",
    "    for cid in candidate_indices:\n",
    "        cand = candidate_list[cid]\n",
    "        candidate_centers.append(np.array(cand.center))\n",
    "        for iid in cand.intersections:\n",
    "            inter = intersection_objs[iid]\n",
    "            all_intersection_points.append(np.array(inter.point))\n",
    "            all_ray_indices.update(inter.ray_pair)\n",
    "\n",
    "    if not all_ray_indices:\n",
    "        print(\"No rays found for the selected candidates.\")\n",
    "        return\n",
    "\n",
    "    # Shift origin to the first ray's origin\n",
    "    ray_indices_sorted = sorted(list(all_ray_indices))\n",
    "    first_ray_origin = np.array(ray_list[ray_indices_sorted[0]].origin)\n",
    "    def shift(pt):\n",
    "        return np.array(pt) - first_ray_origin\n",
    "\n",
    "    # Create Open3D geometries\n",
    "    geometries = []\n",
    "\n",
    "    # Rays as colored lines\n",
    "    ray_colors = []\n",
    "    ray_lines = []\n",
    "    ray_points = []\n",
    "    ray_length = 10.0  # meters, for visualization\n",
    "\n",
    "    for idx, ridx in enumerate(ray_indices_sorted):\n",
    "        ray = ray_list[ridx]\n",
    "        origin = shift(ray.origin)\n",
    "        direction = np.array(ray.direction)\n",
    "        direction = direction / (np.linalg.norm(direction) + 1e-8)\n",
    "        end = origin + direction * ray_length\n",
    "        ray_points.append(origin)\n",
    "        ray_points.append(end)\n",
    "        ray_lines.append([2*idx, 2*idx+1])\n",
    "        ray_colors.append([0.2, 0.2, 1.0])  # blue\n",
    "\n",
    "    if ray_points:\n",
    "        line_set = o3d.geometry.LineSet()\n",
    "        line_set.points = o3d.utility.Vector3dVector(np.array(ray_points))\n",
    "        line_set.lines = o3d.utility.Vector2iVector(np.array(ray_lines))\n",
    "        line_set.colors = o3d.utility.Vector3dVector(np.array(ray_colors * len(ray_lines)))\n",
    "        geometries.append(line_set)\n",
    "\n",
    "    # Intersection points as red spheres\n",
    "    for pt in all_intersection_points:\n",
    "        mesh_sphere = o3d.geometry.TriangleMesh.create_sphere(radius=0.15)\n",
    "        mesh_sphere.translate(shift(pt))\n",
    "        mesh_sphere.paint_uniform_color([1.0, 0.0, 0.0])\n",
    "        geometries.append(mesh_sphere)\n",
    "\n",
    "    # Candidate centers as green spheres\n",
    "    for pt in candidate_centers:\n",
    "        mesh_sphere = o3d.geometry.TriangleMesh.create_sphere(radius=0.25)\n",
    "        mesh_sphere.translate(shift(pt))\n",
    "        mesh_sphere.paint_uniform_color([0.0, 1.0, 0.0])\n",
    "        geometries.append(mesh_sphere)\n",
    "\n",
    "    # Visualize\n",
    "    o3d.visualization.draw_geometries(geometries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26ec6d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_candidates_open3d([390,393,394,395,399])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c209cb2",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6d034ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "range_limit = 20\n",
    "pred_path = '/mnt/Data/StreetView/scripts/triangulation_points_pred_20m.gpkg'\n",
    "traject = pd.read_csv('/mnt/Data/StreetView/data/neuchatel/ne_traject.csv')\n",
    "# Construct geometry from x_m_, y_m_, z_m_\n",
    "traject['geometry'] = traject.apply(lambda row: Point(row['x_m_'], row['y_m_'], row['z_m_']), axis=1)\n",
    "\n",
    "# Also create a 2D geometry column for XY plane\n",
    "traject['geometry_xy'] = traject.apply(lambda row: Point(row['x_m_'], row['y_m_']), axis=1)\n",
    "\n",
    "gt_path = '/mnt/Data/StreetView/data/neuchatel/NE_GT_3D.gpkg'\n",
    "gt_gdf = gpd.read_file(gt_path, layer='ne_gt_3d')\n",
    "\n",
    "# --- Matrix calculation for filtering gt_gdf by nearby traject points in XY ---\n",
    "\n",
    "# Get centroid XY coordinates for all GT geometries\n",
    "gt_centroids = gt_gdf.geometry.centroid\n",
    "gt_centroids_xy = np.array([[pt.x, pt.y] for pt in gt_centroids])\n",
    "\n",
    "# Get all traject XY coordinates\n",
    "traject_xy = np.array([[pt.x, pt.y] for pt in traject['geometry_xy']])\n",
    "\n",
    "# Compute distance matrix: shape (n_gt, n_traject)\n",
    "dists_matrix = np.linalg.norm(gt_centroids_xy[:, None, :] - traject_xy[None, :, :], axis=2)\n",
    "\n",
    "# Count how many traject points are within 15 meters for each GT centroid\n",
    "nearby_counts = (dists_matrix <= range_limit).sum(axis=1)\n",
    "\n",
    "# Filter gt_gdf: keep only rows where at least 3 traject points are within 15m\n",
    "gt_gdf = gt_gdf[nearby_counts >= 3].reset_index(drop=True)\n",
    "\n",
    "\n",
    "pred_gdf = gpd.read_file(pred_path)\n",
    "pred_gdf = pred_gdf[(pred_gdf.mean_intersection_length <= range_limit).values & (pred_gdf.mean_intersection_dist <= 0.1).values]\n",
    "\n",
    "# --- Matrix calculation for mean nearest traject distance in 3D ---\n",
    "\n",
    "# Prepare pred_gdf XYZ coordinates\n",
    "pred_xyz = np.stack([pred_gdf.geometry.x.values, pred_gdf.geometry.y.values, pred_gdf.elevation.values], axis=1)\n",
    "# Prepare traject XYZ coordinates\n",
    "traject_xyz = np.array([[pt.x, pt.y, pt.z] for pt in traject['geometry']])\n",
    "\n",
    "# Compute distance matrix: shape (n_pred, n_traject)\n",
    "dists_3d_matrix = np.linalg.norm(pred_xyz[:, None, :] - traject_xyz[None, :, :], axis=2)\n",
    "\n",
    "# For each pred point, get mean of nearest 5 traject distances\n",
    "k = 10\n",
    "partitioned = np.partition(dists_3d_matrix, k-1, axis=1)[:, :k]\n",
    "mean_nearest_traject_dist = partitioned.mean(axis=1)\n",
    "\n",
    "pred_gdf = pred_gdf.assign(mean_nearest_traject_dist=mean_nearest_traject_dist)\n",
    "pred_gdf = pred_gdf[pred_gdf['mean_intersection_length'] <= 5 + pred_gdf['mean_nearest_traject_dist']].copy()\n",
    "\n",
    "# pred_gdf.to_file(f\"triangulation_points_pred_demo.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a12691e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 1185\n",
      "False Positives: 162\n",
      "False Negatives: 203\n",
      "Precision: 0.880\n",
      "Recall: 0.854\n",
      "f1_score: 0.867\n",
      "Distance statistics (meters) between predicted point and GT polygon centroid in XY plane:\n",
      "  mean: 0.061965935666899535\n",
      "  std: 0.043294714196046004\n",
      "  min: 0.001675845673727821\n",
      "  max: 0.3331082248863427\n",
      "  median: 0.05323019294645722\n",
      "  count: 1185\n"
     ]
    }
   ],
   "source": [
    "# Project GT geometries to XY (2D) polygons\n",
    "gt_polys_2d = gt_gdf['geometry'].apply(lambda geom: geom if geom.geom_type == 'Polygon' else geom.convex_hull)\n",
    "# Project pred points to XY (2D) points\n",
    "pred_points_2d = pred_gdf['geometry'].apply(lambda pt: Point(pt.x, pt.y))\n",
    "\n",
    "# For each GT polygon, find the closest pred point (in XY), and check if it is inside the polygon\n",
    "gt_matched_pred_idx = {}  # gt_idx -> pred_idx if matched\n",
    "pred_matched_gt_idx = {}  # pred_idx -> gt_idx if matched\n",
    "\n",
    "distances = []\n",
    "\n",
    "for gt_idx, gt_poly in gt_polys_2d.items():\n",
    "    # Find closest pred point to this GT polygon (by centroid XY)\n",
    "    gt_centroid = gt_poly.centroid\n",
    "    pred_xy = np.array([[pt.x, pt.y] for pt in pred_points_2d])\n",
    "    gt_xy = np.array([gt_centroid.x, gt_centroid.y])\n",
    "    dists = np.linalg.norm(pred_xy - gt_xy, axis=1)\n",
    "    closest_pred_idx = dists.argmin()\n",
    "    closest_pred_point = pred_points_2d.iloc[closest_pred_idx]\n",
    "    # Check if closest pred point is inside the GT polygon\n",
    "    if gt_poly.contains(closest_pred_point):\n",
    "        gt_matched_pred_idx[gt_idx] = closest_pred_idx\n",
    "        pred_matched_gt_idx[closest_pred_idx] = gt_idx\n",
    "        # Save XY distance for stats\n",
    "        distances.append(np.linalg.norm([closest_pred_point.x - gt_centroid.x, closest_pred_point.y - gt_centroid.y]))\n",
    "\n",
    "# True Positives: number of matched GT polygons (one-to-one)\n",
    "TP = len(gt_matched_pred_idx)\n",
    "# False Positives: pred points not matched to any GT polygon\n",
    "FP = len(pred_points_2d) - len(pred_matched_gt_idx)\n",
    "# False Negatives: GT polygons not matched to any pred point\n",
    "FN = len(gt_polys_2d) - len(gt_matched_pred_idx)\n",
    "\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "# Distance statistics\n",
    "if distances:\n",
    "    distances_np = np.array(distances)\n",
    "    dist_stats = {\n",
    "        'mean': np.mean(distances_np),\n",
    "        'std': np.std(distances_np),\n",
    "        'min': np.min(distances_np),\n",
    "        'max': np.max(distances_np),\n",
    "        'median': np.median(distances_np),\n",
    "        'count': len(distances_np)\n",
    "    }\n",
    "else:\n",
    "    dist_stats = {}\n",
    "\n",
    "print(\"True Positives:\", TP)\n",
    "print(\"False Positives:\", FP)\n",
    "print(\"False Negatives:\", FN)\n",
    "print(\"Precision: {:.3f}\".format(precision))\n",
    "print(\"Recall: {:.3f}\".format(recall))\n",
    "print(\"f1_score: {:.3f}\".format(f1_score))\n",
    "print(\"Distance statistics (meters) between predicted point and GT polygon centroid in XY plane:\")\n",
    "for k, v in dist_stats.items():\n",
    "    print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f87b3a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 1131\n",
      "False Positives: 133\n",
      "False Negatives: 257\n",
      "Precision: 0.895\n",
      "Recall: 0.815\n",
      "f1_score: 0.853\n",
      "Distance statistics (meters) between predicted point and GT polygon centroid in XY plane:\n",
      "  mean: 0.060989909553414094\n",
      "  std: 0.042109058388992045\n",
      "  min: 0.001675845673727821\n",
      "  max: 0.2968191938137795\n",
      "  median: 0.05305985676038003\n",
      "  count: 1131\n"
     ]
    }
   ],
   "source": [
    "# Project GT geometries to XY (2D) polygons\n",
    "gt_polys_2d = gt_gdf['geometry'].apply(lambda geom: geom if geom.geom_type == 'Polygon' else geom.convex_hull)\n",
    "# Project pred points to XY (2D) points\n",
    "pred_points_2d = pred_gdf['geometry'].apply(lambda pt: Point(pt.x, pt.y))\n",
    "\n",
    "# For each GT polygon, find the closest pred point (in XY), and check if it is inside the polygon\n",
    "gt_matched_pred_idx = {}  # gt_idx -> pred_idx if matched\n",
    "pred_matched_gt_idx = {}  # pred_idx -> gt_idx if matched\n",
    "\n",
    "distances = []\n",
    "\n",
    "for gt_idx, gt_poly in gt_polys_2d.items():\n",
    "    # Find closest pred point to this GT polygon (by centroid XY)\n",
    "    gt_centroid = gt_poly.centroid\n",
    "    pred_xy = np.array([[pt.x, pt.y] for pt in pred_points_2d])\n",
    "    gt_xy = np.array([gt_centroid.x, gt_centroid.y])\n",
    "    dists = np.linalg.norm(pred_xy - gt_xy, axis=1)\n",
    "    closest_pred_idx = dists.argmin()\n",
    "    closest_pred_point = pred_points_2d.iloc[closest_pred_idx]\n",
    "    # Check if closest pred point is inside the GT polygon\n",
    "    if gt_poly.contains(closest_pred_point):\n",
    "        gt_matched_pred_idx[gt_idx] = closest_pred_idx\n",
    "        pred_matched_gt_idx[closest_pred_idx] = gt_idx\n",
    "        # Save XY distance for stats\n",
    "        distances.append(np.linalg.norm([closest_pred_point.x - gt_centroid.x, closest_pred_point.y - gt_centroid.y]))\n",
    "\n",
    "# True Positives: number of matched GT polygons (one-to-one)\n",
    "TP = len(gt_matched_pred_idx)\n",
    "# False Positives: pred points not matched to any GT polygon\n",
    "FP = len(pred_points_2d) - len(pred_matched_gt_idx)\n",
    "# False Negatives: GT polygons not matched to any pred point\n",
    "FN = len(gt_polys_2d) - len(gt_matched_pred_idx)\n",
    "\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "# Distance statistics\n",
    "if distances:\n",
    "    distances_np = np.array(distances)\n",
    "    dist_stats = {\n",
    "        'mean': np.mean(distances_np),\n",
    "        'std': np.std(distances_np),\n",
    "        'min': np.min(distances_np),\n",
    "        'max': np.max(distances_np),\n",
    "        'median': np.median(distances_np),\n",
    "        'count': len(distances_np)\n",
    "    }\n",
    "else:\n",
    "    dist_stats = {}\n",
    "\n",
    "print(\"True Positives:\", TP)\n",
    "print(\"False Positives:\", FP)\n",
    "print(\"False Negatives:\", FN)\n",
    "print(\"Precision: {:.3f}\".format(precision))\n",
    "print(\"Recall: {:.3f}\".format(recall))\n",
    "print(\"f1_score: {:.3f}\".format(f1_score))\n",
    "print(\"Distance statistics (meters) between predicted point and GT polygon centroid in XY plane:\")\n",
    "for k, v in dist_stats.items():\n",
    "    print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27212c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 1204\n",
      "False Positives: 197\n",
      "False Negatives: 184\n",
      "Precision: 0.859\n",
      "Recall: 0.867\n",
      "f1_score: 0.863\n",
      "Distance statistics (meters) between predicted point and GT polygon centroid in XY plane:\n",
      "  mean: 0.062448262938878875\n",
      "  std: 0.043845324651224746\n",
      "  min: 0.001675845673727821\n",
      "  max: 0.3331082248863427\n",
      "  median: 0.053422706076573165\n",
      "  count: 1204\n"
     ]
    }
   ],
   "source": [
    "# Project GT geometries to XY (2D) polygons\n",
    "gt_polys_2d = gt_gdf['geometry'].apply(lambda geom: geom if geom.geom_type == 'Polygon' else geom.convex_hull)\n",
    "# Project pred points to XY (2D) points\n",
    "pred_points_2d = pred_gdf['geometry'].apply(lambda pt: Point(pt.x, pt.y))\n",
    "\n",
    "# For each GT polygon, find the closest pred point (in XY), and check if it is inside the polygon\n",
    "gt_matched_pred_idx = {}  # gt_idx -> pred_idx if matched\n",
    "pred_matched_gt_idx = {}  # pred_idx -> gt_idx if matched\n",
    "\n",
    "distances = []\n",
    "\n",
    "for gt_idx, gt_poly in gt_polys_2d.items():\n",
    "    # Find closest pred point to this GT polygon (by centroid XY)\n",
    "    gt_centroid = gt_poly.centroid\n",
    "    pred_xy = np.array([[pt.x, pt.y] for pt in pred_points_2d])\n",
    "    gt_xy = np.array([gt_centroid.x, gt_centroid.y])\n",
    "    dists = np.linalg.norm(pred_xy - gt_xy, axis=1)\n",
    "    closest_pred_idx = dists.argmin()\n",
    "    closest_pred_point = pred_points_2d.iloc[closest_pred_idx]\n",
    "    # Check if closest pred point is inside the GT polygon\n",
    "    if gt_poly.contains(closest_pred_point):\n",
    "        gt_matched_pred_idx[gt_idx] = closest_pred_idx\n",
    "        pred_matched_gt_idx[closest_pred_idx] = gt_idx\n",
    "        # Save XY distance for stats\n",
    "        distances.append(np.linalg.norm([closest_pred_point.x - gt_centroid.x, closest_pred_point.y - gt_centroid.y]))\n",
    "\n",
    "# True Positives: number of matched GT polygons (one-to-one)\n",
    "TP = len(gt_matched_pred_idx)\n",
    "# False Positives: pred points not matched to any GT polygon\n",
    "FP = len(pred_points_2d) - len(pred_matched_gt_idx)\n",
    "# False Negatives: GT polygons not matched to any pred point\n",
    "FN = len(gt_polys_2d) - len(gt_matched_pred_idx)\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "# Distance statistics\n",
    "if distances:\n",
    "    distances_np = np.array(distances)\n",
    "    dist_stats = {\n",
    "        'mean': np.mean(distances_np),\n",
    "        'std': np.std(distances_np),\n",
    "        'min': np.min(distances_np),\n",
    "        'max': np.max(distances_np),\n",
    "        'median': np.median(distances_np),\n",
    "        'count': len(distances_np)\n",
    "    }\n",
    "else:\n",
    "    dist_stats = {}\n",
    "\n",
    "print(\"True Positives:\", TP)\n",
    "print(\"False Positives:\", FP)\n",
    "print(\"False Negatives:\", FN)\n",
    "print(\"Precision: {:.3f}\".format(precision))\n",
    "print(\"Recall: {:.3f}\".format(recall))\n",
    "print(\"f1_score: {:.3f}\".format(f1_score))\n",
    "print(\"Distance statistics (meters) between predicted point and GT polygon centroid in XY plane:\")\n",
    "for k, v in dist_stats.items():\n",
    "    print(f\"  {k}: {v}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
